{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WebSafety Multilingual Text Classifier - Training Notebook\n",
                "\n",
                "Fine-tuning XLM-RoBERTa on 9,000 multilingual web safety samples\n",
                "\n",
                "**Dataset**: WebSafety 9K (English, Hinglish, Telenglish)  \n",
                "**Model**: XLM-RoBERTa-base  \n",
                "**Task**: Multi-class text classification (7 categories)\n",
                "\n",
                "## ‚ö†Ô∏è Important: Enable GPU!\n",
                "Settings ‚Üí Accelerator ‚Üí **GPU T4 x2**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check GPU availability\n",
                "!nvidia-smi"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install -q transformers datasets sentencepiece accelerate scikit-learn matplotlib seaborn"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Imports\n",
                "import json\n",
                "import torch\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from pathlib import Path\n",
                "from sklearn.metrics import classification_report, confusion_matrix, f1_score\n",
                "from transformers import (\n",
                "    XLMRobertaTokenizer, \n",
                "    XLMRobertaForSequenceClassification,\n",
                "    Trainer, \n",
                "    TrainingArguments,\n",
                "    EarlyStoppingCallback\n",
                ")\n",
                "from datasets import Dataset\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "LABEL_MAP = {\n",
                "    \"safe\": 0,\n",
                "    \"phishing\": 1,\n",
                "    \"malware\": 2,\n",
                "    \"hate_speech\": 3,\n",
                "    \"cyberbullying\": 4,\n",
                "    \"sexual_content\": 5,\n",
                "    \"violence\": 6\n",
                "}\n",
                "\n",
                "ID_TO_LABEL = {v: k for k, v in LABEL_MAP.items()}\n",
                "\n",
                "# File paths - UPDATE THESE to match your dataset!\n",
                "TRAIN_FILE = \"/kaggle/input/websafety-9k/train_9k.jsonl\"\n",
                "VAL_FILE = \"/kaggle/input/websafety-9k/validation_9k.jsonl\"\n",
                "TEST_FILE = \"/kaggle/input/websafety-9k/test_9k.jsonl\"\n",
                "OUTPUT_DIR = \"/kaggle/working/websafety-xlm-roberta\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset loader\n",
                "def load_jsonl(file_path):\n",
                "    data = []\n",
                "    with open(file_path, 'r', encoding='utf-8') as f:\n",
                "        for line in f:\n",
                "            data.append(json.loads(line))\n",
                "    return data\n",
                "\n",
                "def prepare_dataset(file_path, tokenizer, max_length=256):\n",
                "    print(f\"Loading {file_path}...\")\n",
                "    raw_data = load_jsonl(file_path)\n",
                "    \n",
                "    texts = [item['text'] for item in raw_data]\n",
                "    labels = [LABEL_MAP[item['primary_label']] for item in raw_data]\n",
                "    \n",
                "    dataset = Dataset.from_dict({'text': texts, 'label': labels})\n",
                "    \n",
                "    def tokenize_function(examples):\n",
                "        return tokenizer(\n",
                "            examples['text'],\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            max_length=max_length\n",
                "        )\n",
                "    \n",
                "    tokenized = dataset.map(tokenize_function, batched=True)\n",
                "    print(f\"  ‚úì Loaded {len(tokenized)} samples\")\n",
                "    return tokenized"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer and prepare datasets\n",
                "print(\"Loading XLM-RoBERTa tokenizer...\")\n",
                "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
                "\n",
                "train_dataset = prepare_dataset(TRAIN_FILE, tokenizer)\n",
                "val_dataset = prepare_dataset(VAL_FILE, tokenizer)\n",
                "test_dataset = prepare_dataset(TEST_FILE, tokenizer)\n",
                "\n",
                "print(f\"\\nDataset sizes:\")\n",
                "print(f\"  Train: {len(train_dataset)}\")\n",
                "print(f\"  Validation: {len(val_dataset)}\")\n",
                "print(f\"  Test: {len(test_dataset)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load model\n",
                "print(\"Loading XLM-RoBERTa model...\")\n",
                "model = XLMRobertaForSequenceClassification.from_pretrained(\n",
                "    'xlm-roberta-base',\n",
                "    num_labels=len(LABEL_MAP),\n",
                "    problem_type=\"single_label_classification\"\n",
                ")\n",
                "\n",
                "print(f\"Model loaded with {sum(p.numel() for p in model.parameters()):,} parameters\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Metrics function\n",
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "    predictions = np.argmax(predictions, axis=1)\n",
                "    \n",
                "    accuracy = (predictions == labels).mean()\n",
                "    f1_macro = f1_score(labels, predictions, average='macro')\n",
                "    f1_weighted = f1_score(labels, predictions, average='weighted')\n",
                "    \n",
                "    return {\n",
                "        'accuracy': accuracy,\n",
                "        'f1_macro': f1_macro,\n",
                "        'f1_weighted': f1_weighted\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=OUTPUT_DIR,\n",
                "    evaluation_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    num_train_epochs=4,\n",
                "    weight_decay=0.01,\n",
                "    warmup_steps=500,\n",
                "    logging_dir=f'{OUTPUT_DIR}/logs',\n",
                "    logging_steps=100,\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model='f1_macro',\n",
                "    greater_is_better=True,\n",
                "    save_total_limit=2,\n",
                "    fp16=True,  # Mixed precision\n",
                "    report_to='none'\n",
                ")\n",
                "\n",
                "# Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=val_dataset,\n",
                "    compute_metrics=compute_metrics,\n",
                "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train!\n",
                "print(\"üèãÔ∏è Starting training...\\n\")\n",
                "trainer.train()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "print(\"\\nüìä Evaluating on test set...\")\n",
                "test_results = trainer.evaluate(test_dataset)\n",
                "\n",
                "print(\"\\nTest Results:\")\n",
                "for key, value in test_results.items():\n",
                "    print(f\"  {key}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Detailed classification report\n",
                "predictions = trainer.predict(test_dataset)\n",
                "y_pred = np.argmax(predictions.predictions, axis=1)\n",
                "y_true = predictions.label_ids\n",
                "\n",
                "print(\"\\nüìã Classification Report:\")\n",
                "print(classification_report(\n",
                "    y_true, \n",
                "    y_pred, \n",
                "    target_names=list(LABEL_MAP.keys()),\n",
                "    digits=4\n",
                "))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix visualization\n",
                "cm = confusion_matrix(y_true, y_pred)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(\n",
                "    cm, \n",
                "    annot=True, \n",
                "    fmt='d', \n",
                "    cmap='Blues',\n",
                "    xticklabels=list(LABEL_MAP.keys()),\n",
                "    yticklabels=list(LABEL_MAP.keys())\n",
                ")\n",
                "plt.title('Confusion Matrix - WebSafety Classifier', fontsize=14, fontweight='bold')\n",
                "plt.ylabel('True Label', fontsize=12)\n",
                "plt.xlabel('Predicted Label', fontsize=12)\n",
                "plt.tight_layout()\n",
                "plt.savefig(f'{OUTPUT_DIR}/confusion_matrix.png', dpi=300)\n",
                "plt.show()\n",
                "\n",
                "print(\"‚úì Confusion matrix saved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model and tokenizer\n",
                "print(\"üíæ Saving model...\")\n",
                "trainer.save_model(OUTPUT_DIR)\n",
                "tokenizer.save_pretrained(OUTPUT_DIR)\n",
                "\n",
                "# Save label mapping\n",
                "with open(f'{OUTPUT_DIR}/label_mapping.json', 'w') as f:\n",
                "    json.dump(LABEL_MAP, f, indent=2)\n",
                "\n",
                "print(f\"\\n‚úÖ Training complete!\")\n",
                "print(f\"üìÅ Model saved to: {OUTPUT_DIR}\")\n",
                "print(\"\\nüéâ Ready for deployment!\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Test predictions on sample texts\n",
                "test_samples = [\n",
                "    \"Had a great time at the beach today!\",\n",
                "    \"You're so ugly, nobody likes you\",\n",
                "    \"Your account has been locked! Click here to verify\",\n",
                "    \"Yaar, ye movie bahut acchi thi!\",\n",
                "    \"Abbai, ee movie chala bagundi!\"\n",
                "]\n",
                "\n",
                "print(\"\\nüß™ Testing on sample texts:\\n\")\n",
                "for text in test_samples:\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=256)\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "        predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
                "        predicted_class = torch.argmax(predictions, dim=1).item()\n",
                "        confidence = predictions[0][predicted_class].item()\n",
                "    \n",
                "    print(f\"Text: {text}\")\n",
                "    print(f\"Prediction: {ID_TO_LABEL[predicted_class]} (confidence: {confidence:.2%})\\n\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}