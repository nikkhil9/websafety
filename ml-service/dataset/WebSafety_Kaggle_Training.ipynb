{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# WebSafety Custom Model Training on Kaggle\n",
                "\n",
                "**Novel Multi-Indic Language Web Safety Dataset with Hinglish & Tenglish**\n",
                "\n",
                "This notebook trains a custom model on the WebSafety dataset with:\n",
                "- 7 primary categories (Safe, Phishing, Malware, Hate Speech, Cyberbullying, Sexual Content, Violence)\n",
                "- Multi-lingual support (English + Hinglish + Tenglish)\n",
                "- Rich contextual metadata\n",
                "\n",
                "## Setup Instructions:\n",
                "\n",
                "1. **Upload your dataset files** to Kaggle:\n",
                "   - Upload `train.jsonl`, `validation.jsonl`, `test.jsonl` as a Kaggle Dataset\n",
                "   - Or add them as notebook input\n",
                "\n",
                "2. **Enable GPU**:\n",
                "   - Go to Settings â†’ Accelerator â†’ GPU T4 x2 (FREE!)\n",
                "\n",
                "3. **Run all cells**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install required packages\n",
                "!pip install transformers datasets torch scikit-learn -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import torch\n",
                "from torch.utils.data import Dataset\n",
                "from transformers import (\n",
                "    AutoTokenizer,\n",
                "    AutoModelForSequenceClassification,\n",
                "    TrainingArguments,\n",
                "    Trainer\n",
                ")\n",
                "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
                "import numpy as np\n",
                "\n",
                "print(\"âœ“ Imports successful\")\n",
                "print(f\"PyTorch version: {torch.__version__}\")\n",
                "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Load Dataset\n",
                "\n",
                "Update the paths below to match where you uploaded your files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# UPDATE THESE PATHS!\n",
                "TRAIN_FILE = \"/kaggle/input/websafety-dataset/train.jsonl\"\n",
                "VAL_FILE = \"/kaggle/input/websafety-dataset/validation.jsonl\"\n",
                "TEST_FILE = \"/kaggle/input/websafety-dataset/test.jsonl\"\n",
                "\n",
                "# Or if files are in working directory:\n",
                "# TRAIN_FILE = \"train.jsonl\"\n",
                "# VAL_FILE = \"validation.jsonl\"\n",
                "# TEST_FILE = \"test.jsonl\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Label mapping\n",
                "LABEL2ID = {\n",
                "    'safe': 0,\n",
                "    'phishing': 1,\n",
                "    'malware': 2,\n",
                "    'hate_speech': 3,\n",
                "    'cyberbullying': 4,\n",
                "    'sexual_content': 5,\n",
                "    'violence': 6\n",
                "}\n",
                "ID2LABEL = {v: k for k, v in LABEL2ID.items()}\n",
                "\n",
                "print(\"Label mapping:\")\n",
                "for label, idx in LABEL2ID.items():\n",
                "    print(f\"  {idx}: {label}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class WebSafetyDataset(Dataset):\n",
                "    \"\"\"Custom Dataset for WebSafety\"\"\"\n",
                "    \n",
                "    def __init__(self, filepath, tokenizer, max_length=512):\n",
                "        self.samples = []\n",
                "        with open(filepath, 'r', encoding='utf-8') as f:\n",
                "            for line in f:\n",
                "                self.samples.append(json.loads(line))\n",
                "        \n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.samples)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        sample = self.samples[idx]\n",
                "        text = sample['text']\n",
                "        label = LABEL2ID[sample['primary_label']]\n",
                "        \n",
                "        encoding = self.tokenizer(\n",
                "            text,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].flatten(),\n",
                "            'attention_mask': encoding['attention_mask'].flatten(),\n",
                "            'labels': torch.tensor(label, dtype=torch.long)\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load tokenizer and model\n",
                "MODEL_NAME = \"distilbert-base-uncased\"  # Fast and good for starter\n",
                "# For multilingual support, use: \"bert-base-multilingual-cased\"\n",
                "\n",
                "print(f\"Loading {MODEL_NAME}...\")\n",
                "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
                "model = AutoModelForSequenceClassification.from_pretrained(\n",
                "    MODEL_NAME,\n",
                "    num_labels=7,\n",
                "    id2label=ID2LABEL,\n",
                "    label2id=LABEL2ID\n",
                ")\n",
                "print(\"âœ“ Model loaded\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create datasets\n",
                "print(\"Loading datasets...\")\n",
                "train_dataset = WebSafetyDataset(TRAIN_FILE, tokenizer)\n",
                "val_dataset = WebSafetyDataset(VAL_FILE, tokenizer)\n",
                "test_dataset = WebSafetyDataset(TEST_FILE, tokenizer)\n",
                "\n",
                "print(f\"âœ“ Train: {len(train_dataset)} samples\")\n",
                "print(f\"âœ“ Validation: {len(val_dataset)} samples\")\n",
                "print(f\"âœ“ Test: {len(test_dataset)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_metrics(pred):\n",
                "    \"\"\"Compute evaluation metrics\"\"\"\n",
                "    labels = pred.label_ids\n",
                "    preds = pred.predictions.argmax(-1)\n",
                "    \n",
                "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
                "        labels, preds, average='weighted', zero_division=0\n",
                "    )\n",
                "    acc = accuracy_score(labels, preds)\n",
                "    \n",
                "    return {\n",
                "        'accuracy': acc,\n",
                "        'f1': f1,\n",
                "        'precision': precision,\n",
                "        'recall': recall\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Training Configuration\n",
                "\n",
                "Adjust these parameters based on your needs:\n",
                "- `num_train_epochs`: More epochs = better performance (but slower)\n",
                "- `per_device_train_batch_size`: Larger = faster (but needs more GPU memory)\n",
                "- `learning_rate`: Lower = more stable, higher = faster convergence"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training arguments\n",
                "training_args = TrainingArguments(\n",
                "    output_dir=\"./websafety-model\",\n",
                "    num_train_epochs=3,\n",
                "    per_device_train_batch_size=16,\n",
                "    per_device_eval_batch_size=16,\n",
                "    learning_rate=2e-5,\n",
                "    warmup_steps=100,\n",
                "    weight_decay=0.01,\n",
                "    logging_steps=50,\n",
                "    eval_strategy=\"epoch\",\n",
                "    save_strategy=\"epoch\",\n",
                "    load_best_model_at_end=True,\n",
                "    metric_for_best_model=\"f1\",\n",
                "    greater_is_better=True,\n",
                "    save_total_limit=2,\n",
                "    report_to=\"none\",  # Disable wandb\n",
                "    fp16=True,  # Use mixed precision for speed\n",
                ")\n",
                "\n",
                "# Create Trainer\n",
                "trainer = Trainer(\n",
                "    model=model,\n",
                "    args=training_args,\n",
                "    train_dataset=train_dataset,\n",
                "    eval_dataset=val_dataset,\n",
                "    compute_metrics=compute_metrics,\n",
                ")\n",
                "\n",
                "print(\"âœ“ Trainer configured\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Start Training!\n",
                "\n",
                "This will take 5-15 minutes depending on GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\" * 60)\n",
                "print(\"STARTING TRAINING\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "trainer.train()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TRAINING COMPLETE!\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate on test set\n",
                "print(\"Evaluating on test set...\")\n",
                "results = trainer.evaluate(test_dataset)\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"TEST SET RESULTS\")\n",
                "print(\"=\"*60)\n",
                "for key, value in results.items():\n",
                "    print(f\"{key}: {value:.4f}\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate predictions for detailed analysis\n",
                "predictions = trainer.predict(test_dataset)\n",
                "pred_labels = predictions.predictions.argmax(-1)\n",
                "true_labels = predictions.label_ids\n",
                "\n",
                "# Classification report\n",
                "print(\"\\nDetailed Classification Report:\")\n",
                "print(\"=\"*60)\n",
                "print(classification_report(\n",
                "    true_labels, \n",
                "    pred_labels, \n",
                "    target_names=list(LABEL2ID.keys())\n",
                "))\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save Model\n",
                "\n",
                "Save your trained model for later use"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model and tokenizer\n",
                "output_dir = \"./websafety-final-model\"\n",
                "trainer.save_model(output_dir)\n",
                "tokenizer.save_pretrained(output_dir)\n",
                "\n",
                "# Save label mapping\n",
                "with open(f\"{output_dir}/label_mapping.json\", 'w') as f:\n",
                "    json.dump({'label2id': LABEL2ID, 'id2label': ID2LABEL}, f, indent=2)\n",
                "\n",
                "print(f\"âœ“ Model saved to {output_dir}\")\n",
                "print(\"âœ“ Download the folder to use in your application!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Test with Custom Examples"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def predict_text(text):\n",
                "    \"\"\"Predict label for custom text\"\"\"\n",
                "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
                "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        outputs = model(**inputs)\n",
                "    \n",
                "    probs = torch.softmax(outputs.logits, dim=-1)\n",
                "    pred_label = probs.argmax(-1).item()\n",
                "    confidence = probs.max().item()\n",
                "    \n",
                "    return ID2LABEL[pred_label], confidence\n",
                "\n",
                "# Test examples\n",
                "test_examples = [\n",
                "    \"This is a great movie, everyone should watch it!\",\n",
                "    \"Click here to claim your prize: http://fake-site.tk\",\n",
                "    \"You're such a loser, nobody likes you\",\n",
                "    \"Yaar, ye website safe hai kya?\",  # Hinglish\n",
                "    \"Abbai, ee link click cheyakandi\",  # Tenglish\n",
                "]\n",
                "\n",
                "print(\"\\nTesting with custom examples:\")\n",
                "print(\"=\"*60)\n",
                "for text in test_examples:\n",
                "    label, conf = predict_text(text)\n",
                "    print(f\"Text: {text[:50]}...\")\n",
                "    print(f\"Prediction: {label} (confidence: {conf:.3f})\")\n",
                "    print(\"-\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸŽ‰ SUCCESS!\n",
                "\n",
                "Your model is now trained! \n",
                "\n",
                "**Next steps:**\n",
                "1. Download the `websafety-final-model` folder\n",
                "2. Use it in your WebSafety application\n",
                "3. Document the results for your research paper\n",
                "\n",
                "**For your paper, report:**\n",
                "- Test accuracy, F1, precision, recall\n",
                "- Per-class performance\n",
                "- Training time and resources\n",
                "- Model architecture (DistilBERT fine-tuned)\n",
                "- Dataset size and distribution"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}